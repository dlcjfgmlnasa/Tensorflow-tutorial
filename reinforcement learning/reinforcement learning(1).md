안녕하십니까 록스(LOCS)에 이철희 입니다

강화학습이라는 개념이 나온 지는 벌써 오래 되었지만, 최근에 중요한 기계학습의 패러다임이 자리를 잡고 나서야 강화학습이 비로소 그 진가를 드러내게 되었습니다. 아타리 게임을 사람보다 잘하고 바둑으로 이세돌 9단을 이기는 등 괄목할만한 성과를 주고 있는 알고리즘이 강화학습입니다.

일반적으로 머신러닝은 아래의 3가지로 분류하며 오늘은 강화학습에 대해 알아봅시다.
- 지도학습
- 비지도학습
- 강화학습

강화학습의 정의는 다음과 같습니다.

> 강화 학습(Reinforcement learning)은 기계학습이 다루는 문제 중에서 다음과 같이 기술 되는 것을 다룬다. 어떤 환경을 탐색하는 에이전트가 현재의 상태를 인식하여 어떤 행동을 취한다. 그러면 그 에이전트는 환경으로부터 포상을 얻게 된다. 포상은 양수와 음수 둘 다 가능하다. 강화 학습의 알고리즘은 그 에이전트가 앞으로 누적될 포상을 최대화하는 일련의 행동으로 정의되는 정책을 찾는 방법이다. [위키백과](https://ko.wikipedia.org/wiki/%EA%B0%95%ED%99%94_%ED%95%99%EC%8A%B5)


위의 문장을 다시 해석해 봅시다.

강화학습은 상호 관계에 대한 고려가 부족했습니다. 상호 관계에 바탕을 둔 강화학습에서는 에이전트(행위자)의 액션(행위)이 환경에 영향을 줍니다.

강화학습은 강화이론을 머신러닝에 적용한 형태라고 이해하면 되며 알고리즘에 따라서 어떤 판단을 했을 때 돌아오는 보상(reward) 에 따라 이 판단의 옳고 그름을 학습하는 것입니다.

강화학습을 진행할 때는 크게 세 세지에 대해 신경습니다.
* state
* action
* reward

state 는 현재의 상태를, action 은 현재 state 에 기반하여 다음에 취하게 될 동작을 나타냅니다. reward 는 각 action 마다 특정한 값으로 보상되며 양수,음수,0의 값으로 돌아옵니다.

생각할 수 있는 그 어떤 과제든 MDP로 표현할 수 있습니다. 예를 들어 문을 여는 액션을 상상해봅니다.

상태는 우리몸의 위치, 그 세계에 존재하는 문, 그리고 우리가 그 문을 시각적으로 어떻게 바라보고 있는지 등이 될 수 있습니다.
액션은 우리 몸이 만들어내는 모든 동작이 되며 보상은 성공적으로 열린 문으로 표현할 수 있습니다.

비록 문으로 다가가는 등의 동작은 문제 해결에 필수적인 액션이기는 하지만, 실제로 문을 여는 액션만이 보상을 제공하기 때문에 그 자체로 보상이 주어지는 액션은 아닙니다. 이 경우 에이전트는 최종적으로 보상으로 이끄는 액션에 가치를 할당하는 법을 학습할 필요가 있기 떄문에 시간의 경과에 따른 역학개념이 필요합니다.